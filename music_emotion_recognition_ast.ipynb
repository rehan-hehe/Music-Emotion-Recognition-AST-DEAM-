{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2362956,"sourceType":"datasetVersion","datasetId":1427290},{"sourceId":11977764,"sourceType":"datasetVersion","datasetId":7532574},{"sourceId":11978173,"sourceType":"datasetVersion","datasetId":7532845},{"sourceId":11980033,"sourceType":"datasetVersion","datasetId":7534235},{"sourceId":418372,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":341287,"modelId":362537},{"sourceId":418495,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":341373,"modelId":362626}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploring the files","metadata":{}},{"cell_type":"code","source":"base_path = \"/kaggle/input/deam-mediaeval-dataset-emotional-analysis-in-music\"\nfor root, dirs, files in os.walk(base_path):\n    print(f\"\\nðŸ“‚ {root}\")\n    for f in files[:5]:  # preview 5 files per folder \n        print(f\"    â””â”€â”€ {f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Importing libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport pickle\nimport torchaudio\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport transformers\nimport librosa\nimport pandas as pd\nimport numpy as np\nimport os\nimport gc\nfrom glob import glob\nfrom torch.utils.data import Dataset,DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom transformers import ASTModel, ASTConfig,ASTFeatureExtractor, Trainer, TrainingArguments\nimport torch.nn as nn\nimport torch.optim as optim # used in defining model optimization functions \nimport torch.nn.functional as F\n\nprint(\"torch:\", torch.__version__)\nprint(\"torchaudio:\", torchaudio.__version__)\nprint(\"transformers:\", transformers.__version__)\nprint(\"librosa:\", librosa.__version__)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Setting up dataframe\n","metadata":{}},{"cell_type":"markdown","source":"## Traversing the data","metadata":{}},{"cell_type":"code","source":"# Path to audio files\naudio_dir = '/kaggle/input/deam-mediaeval-dataset-emotional-analysis-in-music/DEAM_audio/MEMD_audio'\n\n# Paths to annotation CSVs (these have the target variables data)\nann_csv_1 = '/kaggle/input/deam-mediaeval-dataset-emotional-analysis-in-music/DEAM_Annotations/annotations/annotations averaged per song/song_level/static_annotations_averaged_songs_1_2000.csv'\nann_csv_2 = '/kaggle/input/deam-mediaeval-dataset-emotional-analysis-in-music/DEAM_Annotations/annotations/annotations averaged per song/song_level/static_annotations_averaged_songs_2000_2058.csv'\n\n# Loading both CSVs and merging them on common columns\ndf1 = pd.read_csv(ann_csv_1)\ndf2 = pd.read_csv(ann_csv_2, usecols=['song_id', ' valence_mean',' valence_std',' arousal_mean',' arousal_std'])\n# had to get rid of the other columns in the df2 before merging\nlabels_df = pd.concat([df1, df2], ignore_index=True)\n# Creating an empty list to store the paths\naudio_paths = []\n\n# Loop through each song ID and manually build the path\nfor song_id in labels_df['song_id']:\n    full_path = os.path.join(audio_dir, f\"{song_id}.mp3\")\n    if os.path.exists(full_path):\n        audio_paths.append(full_path)\n    else:\n        audio_paths.append(None)  # Just in case the file doesn't exist\n\n# Add this list to the DataFrame\nlabels_df['audio_path'] = audio_paths","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Modifying column names to remove confusion\nlabels_df.rename(columns={' valence_mean':'valence_mean',' valence_std':'valence_std',' arousal_mean':'arousal_mean',' arousal_std':'arousal_std'}, inplace=True)\nprint(labels_df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Studying the first entry \nsample_path = labels_df.iloc[0]['audio_path']\n\n# Load audio\nwaveform, sr = torchaudio.load(sample_path) #the load function returns waveform with sample rate\n# the sample rate came out to be 44100 \nwaveform = waveform.mean(dim=0) #The audio is stereo i.e. separate left and right channel so we merge them using mean\nprint(f\"Sample rate: {sr}, Shape: {waveform.shape}\")\n\n# Plot waveform\nplt.figure(figsize=(10, 3))\nplt.plot(waveform.numpy())\nplt.title('Waveform')\nplt.show()\n\n# Show spectrogram\nspec = torchaudio.transforms.MelSpectrogram(sample_rate=sr)(waveform)\nspec_db = torchaudio.transforms.AmplitudeToDB()(spec)\n\nplt.figure(figsize=(10, 4))\nplt.xlabel('Time')\nplt.ylabel('Frequency(in mels)')\nplt.imshow(spec_db.numpy(), aspect='auto', origin='lower')\nplt.title('Mel Spectrogram (dB)')\nplt.colorbar()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"naming functions so we can easily call them later\nsr=44100 #observed from the analysis of one of the entries of the spectrogram\nmel_extractor = torchaudio.transforms.MelSpectrogram(sample_rate=sr) # converts to spectrogram\nto_db = torchaudio.transforms.AmplitudeToDB() #the spectrogram orginally measures the intensity values differently so we convert them into dB for human-interpretability","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#naming functions so we can easily call them later\nsr=44100 #observed from the analysis of one of the entries of the spectrogram\nmel_extractor = torchaudio.transforms.MelSpectrogram(sample_rate=sr) # converts to spectrogram\nto_db = torchaudio.transforms.AmplitudeToDB() #the spectrogram orginally measures the intensity values differently so we convert them into dB for huam-interpretability\n\n# Create empty lists to store features and labels\n# these will house the spectrograms from each file after conversion\nspec_list = []\nvalence_targets = []\narousal_targets = []\n\nfor idx, row in labels_df.iterrows():\n    audio_path = row['audio_path']\n    \n    try:\n        waveform, sr = torchaudio.load(audio_path)\n        waveform = waveform.mean(dim=0)  # convert stereo to mono\n\n        # Convert to Mel Spectrogram in dB\n        spec = mel_extractor(waveform)\n        spec_db = to_db(spec)\n\n        spec_list.append(spec_db)\n        valence_targets.append(row['valence_mean'])\n        arousal_targets.append(row['arousal_mean'])\n    \n    except Exception as e:\n        print(f\"Failed for index {idx} - {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#EDA to analyse which samples can be safely dropped and what should be the trucation level of our spectrograms to maintain uniformity in their lengths\n\nlengths=[]\nfor spec in spec_list:\n    lengths.append(spec.shape[1])\nmax_length = max(lengths)\nmin_length = min(lengths)\n\nprint(f\"Max number of time frames: {max_length}\")\nprint(f\"Min number of time frames: {min_length}\")\n\n\n# Assuming `lengths` is a list or array of spectrogram frame lengths\nlengths_array = np.array(lengths)\n\n# Plot histogram\ncounts, bin_edges, _ = plt.hist(lengths_array, bins=800, edgecolor='black')\nplt.title('Histogram of Spectrogram Time Frame Lengths')\nplt.xlabel('Number of Time Frames')\nplt.ylabel('Number of Samples')\nplt.grid(True)\nplt.show()\n\n# Find the bin with the maximum count\nmax_bin_index = np.argmax(counts)\nmax_bin_range = (bin_edges[max_bin_index], bin_edges[max_bin_index + 1])\nmax_bin_count = counts[max_bin_index]\n\nprint(f\"Most dense bin range: {max_bin_range}\")\nprint(f\"Number of samples in this bin: {int(max_bin_count)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since 1718/1802 samples lie in this bin, we will follow the following methodology for data preprocessing: \n1) we drop the samples that are shorter than 9830.\n2) We truncate the rest to 9830 length.","metadata":{}},{"cell_type":"code","source":"fixed_length = 9830\n\n# Create empty lists to store valid data\nclean_spec_list = []\nclean_valence_targets = []\nclean_arousal_targets = []\nclean_paths = []\n\nfor idx, row in labels_df.iterrows():\n    audio_path = row['audio_path']\n    \n    try:\n        waveform, sr = torchaudio.load(audio_path)\n        waveform = waveform.mean(dim=0)  # stereo â†’ mono\n\n        # Mel Spectrogram in dB\n        spec = mel_extractor(waveform)\n        spec_db = to_db(spec)\n\n        # Only keep if it's long enough\n        if spec_db.shape[1] >= fixed_length:\n            truncated = spec_db[:, :fixed_length]  # Truncate if longer\n            clean_spec_list.append(truncated)\n            clean_valence_targets.append(row['valence_mean'])\n            clean_arousal_targets.append(row['arousal_mean'])\n            clean_paths.append(audio_path)\n        \n    except Exception as e:\n        print(f\"Failed for index {idx} - {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Uncomment if you want to save these files for later use\n\n#SAVING  the lists formed above:\n# os.makedirs(\"/kaggle/working/clean_stuff\", exist_ok=True)\n\n# torch.save(clean_spec_list, \"/kaggle/working/clean_stuff/clean_spec_list.pt\")\n\n# with open(\"/kaggle/working/clean_stuff/clean_valence_targets.pkl\", \"wb\") as f:\n#     pickle.dump(clean_valence_targets, f)\n\n# with open(\"/kaggle/working/clean_stuff/clean_arousal_targets.pkl\", \"wb\") as f:\n#     pickle.dump(clean_arousal_targets, f)\n\n# with open(\"/kaggle/working/clean_stuff/clean_paths.pkl\", \"wb\") as f:\n#     pickle.dump(clean_paths, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#to load later: \n# clean_spec_list = torch.load(\"/kaggle/working/clean_stuff/clean_spec_list.pt\")\n# with open(\"/kaggle/working/clean_stuff/clean_valence_targets.pkl\", \"rb\") as f:\n#     clean_valence_targets = pickle.load(f)\n\n# with open(\"/kaggle/working/clean_stuff/clean_arousal_targets.pkl\", \"rb\") as f:\n#     clean_arousal_targets = pickle.load(f)\n\n# with open(\"/kaggle/working/clean_stuff/clean_paths.pkl\", \"rb\") as f:\n#     clean_paths = pickle.load(f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#making new dataframe out of the clean stuff\nfiltered_labels_df = pd.DataFrame({\n    'audio_path': clean_paths,\n    'valence_mean': clean_valence_targets,\n    'arousal_mean': clean_arousal_targets\n})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# to save the above created df\n# filtered_labels_df.to_csv(\"filtered_labels_df.csv\",index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# to load it\n# filtered_labels_df=pd.read_csv(\"/kaggle/input/filtered-labels-df/filtered_labels_df.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#our ratings were in range [1,9], we normalize them to [0,1]\nnormalized_valence_targets = (filtered_labels_df['valence_mean'].values - 1) / 8\nnormalized_arousal_targets = (filtered_labels_df['arousal_mean'].values - 1) / 8\n# later we can use: valence_pred_original = valence_pred * 8 + 1 to interpret our results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #Saving normalized labels:\n\n# with open(\"/kaggle/working/normalized_valence_targets.pkl\", \"wb\") as f:\n#     pickle.dump(normalized_valence_targets, f)\n\n# with open(\"/kaggle/working/normalized_arousal_targets.pkl\", \"wb\") as f:\n#     pickle.dump(normalized_arousal_targets, f)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Z-score normalization (preferred for transformers over min-max normalization)\n# Step 1: Stack all spectrograms to compute global mean and std\nall_specs = torch.cat([spec.flatten() for spec in clean_spec_list])\nglobal_mean = all_specs.mean()\nglobal_std = all_specs.std()\n\nprint(f\"Global mean: {global_mean:.4f}, Global std: {global_std:.4f}\")\n\n# Step 2: Normalize each spectrogram\nnormalized_spec_list = [(spec - global_mean) / global_std for spec in clean_spec_list]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#SAVING NORMALIZED LIST\n# torch.save(normalized_spec_list, \"/kaggle/working/normalized_spec_list.pt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#LOAD normalized list\n# normalized_spec_list = torch.load(\"/kaggle/input/normalized-spec-listtt/normalized_spec_list.pt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"normalized_spec_list[0].shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# we need to truncate the last dimension such that it is a multiple of 16 because the model uses patch size 16x16\nfor i in range(len(normalized_spec_list)):\n    normalized_spec_list[i] = normalized_spec_list[i][:, :9216]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"normalized_spec_list[0].shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# our transformer needs input with dimension (128,1024), so we break ours down into 9 chunks and increase dataset\nchunk_size = 1024\nnum_chunks = 9\n\n# Final lists\nsegmented_specs = []\nsegmented_valence = []\nsegmented_arousal = []\nfor i in range(len(normalized_spec_list)):\n    spec = normalized_spec_list[i]  # shape: [128, 9216]\n    valence = normalized_valence_targets[i]\n    arousal = normalized_arousal_targets[i]\n\n    for j in range(num_chunks):\n        start = j * chunk_size\n        end = start + chunk_size\n        segment = spec[:, start:end]  # shape: [128, 1024]\n\n        segmented_specs.append(segment)\n        segmented_valence.append(valence)\n        segmented_arousal.append(arousal)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# # LETS START WITH DATASET PREPARATION ","metadata":{}},{"cell_type":"code","source":"class EmotionAudioDataset(Dataset):\n    def __init__(self, specs, labels):\n        self.specs = specs\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.specs)\n\n    def __getitem__(self, idx):\n        spec = torch.tensor(self.specs[idx], dtype=torch.float32)  # [128, 1024]\n        spec = spec.unsqueeze(0)  # Add channel dim: [1, 128, 1024]\n        label = torch.tensor(self.labels[idx], dtype=torch.float32)  # [2]\n        return {\"input_values\": spec, \"labels\": label}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Train test val split: \n# Combine valence and arousal into a tuple of labels\nall_labels = list(zip(segmented_valence, segmented_arousal))\n\n#zip combines the two values in a vector and makes a list out of them\n\n# First split: Train vs Temp (val + test)\nX_train, X_temp, y_train, y_temp = train_test_split(\n    segmented_specs, all_labels, test_size=0.2, random_state=42\n)\n\n# Second split: Validation vs Test\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.5, random_state=42\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = EmotionAudioDataset(X_train, y_train)\nval_dataset = EmotionAudioDataset(X_val, y_val)\ntest_dataset = EmotionAudioDataset(X_test, y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nbatch_size = 16  # decided after multiple iterations\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MODEL IN PROGRESS\n","metadata":{}},{"cell_type":"code","source":"\nclass ASTForEmotionRegression(nn.Module):\n    def __init__(self, pretrained_model_name='MIT/ast-finetuned-audioset-10-10-0.4593'):\n        super().__init__()\n        self.ast = ASTModel.from_pretrained(pretrained_model_name)\n\n        self.regression_head = nn.Sequential(\n            nn.Linear(self.ast.config.hidden_size, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 2)  # Output: [valence, arousal]\n        )\n        \n    def forward(self, input_values):\n        \"\"\"\n        input_values: Tensor of shape [B, 1, 128, 9824]\n        \"\"\"\n        # Flatten to [B, 128, 1024]\n        input_values = input_values.squeeze(1)\n\n        # AST expects inputs_embeds of shape [B, F, T]\n        outputs = self.ast(input_values=input_values)\n        hidden_states = outputs.last_hidden_state  # [B, T_patches, H]\n\n        pooled = hidden_states.mean(dim=1)  # [B, H]\n        output = self.regression_head(pooled)  # [B, 2]\n        return output\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Instantiate model\nmodel = ASTForEmotionRegression()\nmodel.to(device)\n\n# Define loss function\ncriterion = nn.MSELoss()  # Mean Squared Error for regression\n\n# Define optimizer\noptimizer = optim.Adam(model.parameters(), lr=1e-4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nnum_epochs = 15\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n\n    for batch in train_loader:\n        specs = batch[\"input_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(specs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    avg_train_loss = running_loss / len(train_loader)\n\n    # Validation phase\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in val_loader:\n            specs = batch[\"input_values\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            outputs = model(specs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n    avg_val_loss = val_loss / len(val_loader)\n\n    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# torch.save(model.state_dict(), \"ast_emotion_regression_moresamples_batch16_epochs15.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# torch.save(model, \"ast_emotion_regression_full_moresamples_batch16_epochs15.pt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TO directly, use the saved weights of the model, uncomment this \n# model = ASTForEmotionRegression()\n\n# # Step 3: Load the weights\n# state_dict = torch.load(\"/kaggle/input/ast_emotion_detection/pytorch/default/1/ast_emotion_regression_moresamples_batch16_epochs10.pth\", map_location=\"cpu\")\n# model.load_state_dict(state_dict)\n\n# # Define loss function\n# criterion = nn.MSELoss()  # Mean Squared Error for regression\n\n# # Define optimizer\n# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n# # Step 4: Move model to device\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\ntest_loss = 0.0\nall_preds = []\nall_targets = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        specs = batch[\"input_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        outputs = model(specs)\n\n        test_loss += criterion(outputs, labels).item()\n\n        all_preds.append(outputs.cpu())\n        all_targets.append(labels.cpu())\n\navg_test_loss = test_loss / len(test_loader)\nprint(f\"Test MSE Loss: {avg_test_loss:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nmodel.eval()\npreds = []\nlabels = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        inputs = batch[\"input_values\"].to(device)\n        targets = batch[\"labels\"].to(device)\n\n        outputs = model(inputs)\n\n        preds.append(outputs.cpu())\n        labels.append(targets.cpu())\n\n# Stack into numpy arrays\npreds = torch.cat(preds).numpy()\nlabels = torch.cat(labels).numpy()\n\n# Compute metrics\nvalence_mse = mean_squared_error(labels[:, 0], preds[:, 0])\narousal_mse = mean_squared_error(labels[:, 1], preds[:, 1])\n\nvalence_mae = mean_absolute_error(labels[:, 0], preds[:, 0])\narousal_mae = mean_absolute_error(labels[:, 1], preds[:, 1])\n\nprint(f\"Valence MSE: {valence_mse:.4f} | MAE: {valence_mae:.4f}\")\nprint(f\"Arousal MSE: {arousal_mse:.4f} | MAE: {arousal_mae:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}